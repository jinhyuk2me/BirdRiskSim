#!/usr/bin/env python3
"""
BirdRiskSim 3D Triangulation Module
- Unity ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
- YOLO ê²°ê³¼ ì‚¼ê°ì¸¡ëŸ‰
- ì‹¤ì‹œê°„ ë° ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent  # scripts/ -> BirdRiskSim_v2/
sys.path.insert(0, str(project_root))

import cv2
import numpy as np
import json
from typing import List, Dict, Optional, Tuple, Union
# ğŸ¯ í•­ê³µ ê°ì§€ í†µí•© ëª¨ë“ˆ import
from aviation_detector import AviationDetector
import pandas as pd
from datetime import datetime
import glob

# =========================
# ğŸ”§ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
# =========================

def load_camera_parameters(json_path: Union[str, Path]) -> Dict:
    """JSON íŒŒì¼ì—ì„œ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œ"""
    with open(json_path, 'r') as f:
        params = json.load(f)
    return params

def calculate_stereo_calibration(params1: Dict, params2: Dict) -> Dict:
    """
    ë‘ ì¹´ë©”ë¼ ê°„ì˜ ì •í™•í•œ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚°
    
    Args:
        params1, params2: ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ (R, T, ìŠ¤ì¼€ì¼ íŒ©í„° ë“±)
    """
    # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    def extract_intrinsic_matrix(params):
        # Unity projectionMatrixì—ì„œ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        proj = params['projectionMatrix']
        width = params['imageWidth']
        height = params['imageHeight']
        
        # Unity projection matrixë¥¼ OpenCV ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜
        fx = proj['m00'] * width / 2
        fy = proj['m11'] * height / 2
        cx = width / 2
        cy = height / 2
        
        K = np.array([
            [fx, 0, cx],
            [0, fy, cy],
            [0, 0, 1]
        ], dtype=np.float32)
        
        return K
    
    # ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (Unity ì›”ë“œ ì¢Œí‘œê³„)
    def extract_extrinsic_params(params):
        pos = params['position_UnityWorld']
        rot = params['rotation_UnityWorld']
        
        # Unity ì¹´ë©”ë¼ ìœ„ì¹˜ (ì›”ë“œ ì¢Œí‘œê³„)
        t_world = np.array([pos['x'], pos['y'], pos['z']], dtype=np.float32)
        
        # Unity ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜ (ì¹´ë©”ë¼ â†’ ì›”ë“œ)
        q = [rot['x'], rot['y'], rot['z'], rot['w']]
        R_cam_to_world = quaternion_to_rotation_matrix_corrected(q)
        
        # ì›”ë“œ â†’ ì¹´ë©”ë¼ ë³€í™˜
        R_world_to_cam = R_cam_to_world.T
        t_world_to_cam = -R_world_to_cam @ t_world
        
        return R_world_to_cam, t_world_to_cam
    
    # ë‚´ë¶€ íŒŒë¼ë¯¸í„°
    K1 = extract_intrinsic_matrix(params1)
    K2 = extract_intrinsic_matrix(params2)
    
    # ì™¸ë¶€ íŒŒë¼ë¯¸í„°
    R1, t1 = extract_extrinsic_params(params1)
    R2, t2 = extract_extrinsic_params(params2)
    
    # ìƒëŒ€ì  íšŒì „ê³¼ í‰í–‰ì´ë™ ê³„ì‚°
    R_rel = R2 @ R1.T  # ì¹´ë©”ë¼1ì—ì„œ ì¹´ë©”ë¼2ë¡œì˜ íšŒì „
    t_rel = t2 - R_rel @ t1  # ìƒëŒ€ì  í‰í–‰ì´ë™
    
    # ë² ì´ìŠ¤ë¼ì¸ ê±°ë¦¬ ê³„ì‚°
    baseline = np.linalg.norm(t2 - t1)
    
    # ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚° (Unity ë‹¨ìœ„ ê¸°ì¤€)
    unity_scale = 1.0  # UnityëŠ” ë¯¸í„° ë‹¨ìœ„ ì‚¬ìš©
    
    return {
        'K1': K1, 'K2': K2,
        'R': R_rel, 'T': t_rel,
        'baseline': baseline,
        'scale_factor': unity_scale,
        'camera1_pos': t1,
        'camera2_pos': t2,
        'camera1_rot': R1,
        'camera2_rot': R2
    }

def quaternion_to_rotation_matrix(q):
    """ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    x, y, z, w = q
    
    # Unity ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
    R = np.array([
        [1 - 2*(y*y + z*z), 2*(x*y - z*w), 2*(x*z + y*w)],
        [2*(x*y + z*w), 1 - 2*(x*x + z*z), 2*(y*z - x*w)],
        [2*(x*z - y*w), 2*(y*z + x*w), 1 - 2*(x*x + y*y)]
    ], dtype=np.float32)
    
    return R

def quaternion_to_rotation_matrix_corrected(q):
    """ì˜¬ë°”ë¥¸ ì¿¼í„°ë‹ˆì–¸ â†’ íšŒì „ í–‰ë ¬ ë³€í™˜"""
    x, y, z, w = q
    
    # ì •ê·œí™”
    norm = np.sqrt(x*x + y*y + z*z + w*w)
    if norm > 0:
        x, y, z, w = x/norm, y/norm, z/norm, w/norm
    
    # íšŒì „ í–‰ë ¬ ê³„ì‚°
    R = np.array([
        [1 - 2*(y*y + z*z), 2*(x*y - z*w), 2*(x*z + y*w)],
        [2*(x*y + z*w), 1 - 2*(x*x + z*z), 2*(y*z - x*w)],
        [2*(x*z - y*w), 2*(y*z + x*w), 1 - 2*(x*x + y*y)]
    ], dtype=np.float32)
    
    return R

def triangulate_point_stereo(point1: List[float], point2: List[float], 
                           stereo_calib: Dict) -> Optional[np.ndarray]:
    """
    ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì •ë³´ë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ì‚¼ê°ì¸¡ëŸ‰
    
    Args:
        point1, point2: ì´ë¯¸ì§€ ì¢Œí‘œ [x, y]
        stereo_calib: calculate_stereo_calibration ê²°ê³¼
    
    Returns:
        Unity ì›”ë“œ ì¢Œí‘œ 3D ìœ„ì¹˜ [x, y, z] ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
    """
    try:
        # ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
        K1, K2 = stereo_calib['K1'], stereo_calib['K2']
        
        # ì •ê·œí™”ëœ ì¢Œí‘œ ê³„ì‚°
        point1_norm = np.linalg.inv(K1) @ np.array([point1[0], point1[1], 1])
        point2_norm = np.linalg.inv(K2) @ np.array([point2[0], point2[1], 1])
        
        # ì‚¼ê°ì¸¡ëŸ‰ (DLT ë°©ë²•)
        R, T = stereo_calib['R'], stereo_calib['T']
        
        # íˆ¬ì˜ í–‰ë ¬ êµ¬ì„±
        P1 = K1 @ np.hstack([np.eye(3), np.zeros((3, 1))])  # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ (ê¸°ì¤€)
        P2 = K2 @ np.hstack([R, T.reshape(-1, 1)])  # ë‘ ë²ˆì§¸ ì¹´ë©”ë¼
        
        # OpenCV ì‚¼ê°ì¸¡ëŸ‰
        points1 = np.array([[point1[0]], [point1[1]]], dtype=np.float32)
        points2 = np.array([[point2[0]], [point2[1]]], dtype=np.float32)
        
        points_4d_hom = cv2.triangulatePoints(P1, P2, points1, points2)
        points_3d = (points_4d_hom[:3] / points_4d_hom[3]).flatten()
        
        # ê²°ê³¼ëŠ” ì´ë¯¸ ì²« ë²ˆì§¸ ì¹´ë©”ë¼ ê¸°ì¤€ ì¢Œí‘œê³„ì—ì„œ ê³„ì‚°ë¨
        # ì›”ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•˜ë ¤ë©´ ì²« ë²ˆì§¸ ì¹´ë©”ë¼ì˜ ì—­ë³€í™˜ ì ìš©
        R1, t1 = stereo_calib['camera1_rot'], stereo_calib['camera1_pos']
        
        # ì¹´ë©”ë¼ ì¢Œí‘œê³„ â†’ Unity ì›”ë“œ ì¢Œí‘œê³„ (R1ì€ ì´ë¯¸ world_to_camì´ë¯€ë¡œ ì—­ë³€í™˜)
        point_3d_unity = R1.T @ (points_3d - t1)
        
        # ìŠ¤ì¼€ì¼ íŒ©í„° ì ìš©
        point_3d_unity *= stereo_calib['scale_factor']
        
        print(f"ğŸ”„ ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰: ì¹´ë©”ë¼({points_3d[0]:.1f}, {points_3d[1]:.1f}, {points_3d[2]:.1f}) â†’ Unity({point_3d_unity[0]:.1f}, {point_3d_unity[1]:.1f}, {point_3d_unity[2]:.1f})")
        
        return point_3d_unity
        
    except Exception as e:
        print(f"âŒ ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰ ì˜¤ë¥˜: {e}")
        return None

def get_projection_matrix(params: Dict) -> np.ndarray:
    """
    Unity ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ë¡œë¶€í„° ì˜¬ë°”ë¥¸ OpenCV í˜¸í™˜ íˆ¬ì˜ í–‰ë ¬ ê³„ì‚°
    """
    # 1. Unity ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    proj = params['projectionMatrix']
    width = params['imageWidth']
    height = params['imageHeight']
    
    # Unity projection matrixì—ì„œ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    fx = proj['m00'] * width / 2.0
    fy = proj['m11'] * height / 2.0
    cx = width / 2.0
    cy = height / 2.0
    
    # ë‚´ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬
    K = np.array([
        [fx, 0, cx],
        [0, fy, cy],
        [0, 0, 1]
    ], dtype=np.float32)
    
    # 2. Unity ì™¸ë¶€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì¹´ë©”ë¼ â†’ ì›”ë“œ)
    pos = params['position_UnityWorld']
    rot = params['rotation_UnityWorld']
    
    # Unity ì¹´ë©”ë¼ ìœ„ì¹˜ (ì›”ë“œ ì¢Œí‘œê³„)
    t_world = np.array([pos['x'], pos['y'], pos['z']], dtype=np.float32)
    
    # Unity ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜ (ì¹´ë©”ë¼ â†’ ì›”ë“œ)
    q = [rot['x'], rot['y'], rot['z'], rot['w']]
    R_cam_to_world = quaternion_to_rotation_matrix_corrected(q)
    
    # 3. ì›”ë“œ â†’ ì¹´ë©”ë¼ ë³€í™˜ ê³„ì‚°
    R_world_to_cam = R_cam_to_world.T  # ì—­íšŒì „
    t_world_to_cam = -R_world_to_cam @ t_world  # ì—­ë³€í™˜
    
    # 4. Unity â†’ OpenCV ì¢Œí‘œê³„ ë³€í™˜
    # Unity: Y-up, Z-forward (LHS)
    # OpenCV: Y-up, Z-forward (RHS) - Yì¶• ë°˜ì „ ì œê±°
    unity_to_opencv = np.array([
        [1,  0,  0],   # Xì¶• ê·¸ëŒ€ë¡œ
        [0,  1,  0],   # Yì¶• ê·¸ëŒ€ë¡œ (ë°˜ì „ ì œê±°)
        [0,  0,  1]    # Zì¶• ê·¸ëŒ€ë¡œ
    ], dtype=np.float32)
    
    # OpenCV ì¢Œí‘œê³„ë¡œ ë³€í™˜
    R_opencv = unity_to_opencv @ R_world_to_cam @ unity_to_opencv.T
    t_opencv = unity_to_opencv @ t_world_to_cam
    
    # 5. íˆ¬ì˜ í–‰ë ¬ P = K[R|t]
    Rt = np.hstack([R_opencv, t_opencv.reshape(3, 1)])
    P = K @ Rt
    
    return P

def get_projection_matrix_simple(params: Dict) -> np.ndarray:
    """
    ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ìš© ê°„ë‹¨í•œ íˆ¬ì˜ í–‰ë ¬ ê³„ì‚°
    (ê¸°ì¡´ real_time_pipeline.pyì˜ ë°©ì‹ê³¼ í˜¸í™˜)
    """
    # ë‚´ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬
    K = np.array([
        [params['fx'], 0, params['cx']],
        [0, params['fy'], params['cy']],
        [0, 0, 1]
    ])
    
    # íšŒì „ í–‰ë ¬
    R = np.array(params['rotation_matrix'])
    
    # ì´ë™ ë²¡í„°
    t = np.array(params['translation_vector']).reshape(3, 1)
    
    # ì™¸ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬ [R|t]
    Rt = np.hstack([R, t])
    
    # íˆ¬ì˜ í–‰ë ¬ P = K[R|t]
    P = K @ Rt
    
    return P

# =========================
# ğŸ¯ ê°ì²´ ë§¤ì¹­ ë° ë³‘í•©
# =========================

def merge_nearby_flocks_2d(flocks: List[Tuple], distance_threshold: float = 100) -> List[Tuple]:
    """
    2D ì´ë¯¸ì§€ì—ì„œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìˆëŠ” flockë“¤ì„ í†µí•© (ë°°ì¹˜ ì²˜ë¦¬ìš©)
    
    Args:
        flocks: [(box, conf), ...] í˜•íƒœì˜ flock ë¦¬ìŠ¤íŠ¸
        distance_threshold: í†µí•©í•  ìµœëŒ€ ê±°ë¦¬ (í”½ì…€ ë‹¨ìœ„)
    
    Returns:
        í†µí•©ëœ flock ë¦¬ìŠ¤íŠ¸
    """
    if not flocks:
        return []
        
    # ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚°
    centers = np.array([box[:2] for box, _ in flocks])
    confidences = np.array([conf for _, conf in flocks])
    
    # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
    distances = np.zeros((len(flocks), len(flocks)))
    for i in range(len(flocks)):
        for j in range(i+1, len(flocks)):
            dist = np.linalg.norm(centers[i] - centers[j])
            distances[i,j] = distances[j,i] = dist
    
    # í†µí•©í•  ê·¸ë£¹ ì°¾ê¸°
    merged_groups = []
    used = set()
    
    for i in range(len(flocks)):
        if i in used:
            continue
            
        # í˜„ì¬ flockê³¼ ê°€ê¹Œìš´ flockë“¤ ì°¾ê¸°
        nearby = [j for j in range(len(flocks)) 
                 if distances[i,j] < distance_threshold and j not in used]
        
        if nearby:
            group = [i] + nearby
            merged_groups.append(group)
            used.update(group)
        else:
            merged_groups.append([i])
            used.add(i)
    
    # ê° ê·¸ë£¹ í†µí•©
    merged_flocks = []
    for group in merged_groups:
        if len(group) == 1:
            # ë‹¨ì¼ flockì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            merged_flocks.append(flocks[group[0]])
        else:
            # ì—¬ëŸ¬ flock í†µí•©
            group_boxes = np.array([flocks[i][0] for i in group])
            group_confs = np.array([flocks[i][1] for i in group])
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¤‘ì‹¬ì  ê³„ì‚°
            weights = group_confs / np.sum(group_confs)
            merged_center = np.sum(group_boxes[:, :2] * weights[:, None], axis=0)
            
            # í¬ê¸°ëŠ” ìµœëŒ€ê°’ ì‚¬ìš©
            merged_size = np.max(group_boxes[:, 2:], axis=0)
            
            # ì‹ ë¢°ë„ëŠ” í‰ê·  ì‚¬ìš©
            merged_conf = np.mean(group_confs)
            
            merged_box = np.concatenate([merged_center, merged_size])
            merged_flocks.append((merged_box, merged_conf))
    
    return merged_flocks

def merge_nearby_flocks_3d(points: List[Dict], distance_threshold: float = 100) -> List[Dict]:
    """
    3D ê³µê°„ì—ì„œ ê·¼ì ‘í•œ ë¬´ë¦¬ ë³‘í•© (ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ìš©)
    
    Args:
        points: 3D ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ê°ì²´ ë¦¬ìŠ¤íŠ¸
        distance_threshold: í†µí•©í•  ìµœëŒ€ ê±°ë¦¬ (ë¯¸í„° ë‹¨ìœ„)
    
    Returns:
        í†µí•©ëœ ì  ë¦¬ìŠ¤íŠ¸
    """
    if not points:
        return points
    
    # Flockë§Œ í•„í„°ë§
    flocks = [p for p in points if p['class'] == 'Flock']
    others = [p for p in points if p['class'] != 'Flock']
    
    if len(flocks) <= 1:
        return points
    
    # ê±°ë¦¬ ê¸°ë°˜ ë³‘í•©
    merged_flocks = []
    used_indices = set()
    
    for i, flock1 in enumerate(flocks):
        if i in used_indices:
            continue
        
        # í˜„ì¬ ë¬´ë¦¬ì™€ ë³‘í•©í•  ë¬´ë¦¬ë“¤ ì°¾ê¸°
        merge_group = [flock1]
        used_indices.add(i)
        
        for j, flock2 in enumerate(flocks):
            if j in used_indices:
                continue
            
            # ê±°ë¦¬ ê³„ì‚° (XZ í‰ë©´)
            dist = np.sqrt(
                (flock1['x'] - flock2['x'])**2 + 
                (flock1['z'] - flock2['z'])**2
            )
            
            if dist < distance_threshold:
                merge_group.append(flock2)
                used_indices.add(j)
        
        # ë³‘í•©ëœ ë¬´ë¦¬ì˜ í‰ê·  ìœ„ì¹˜ ê³„ì‚°
        if len(merge_group) > 1:
            avg_x = np.mean([f['x'] for f in merge_group])
            avg_y = np.mean([f['y'] for f in merge_group])
            avg_z = np.mean([f['z'] for f in merge_group])
            avg_conf = np.mean([f['confidence'] for f in merge_group])
            
            merged_flock = {
                'frame': flock1['frame'],
                'class': 'Flock',
                'x': avg_x,
                'y': avg_y,
                'z': avg_z,
                'confidence': avg_conf,
                'cameras': f"merged_{len(merge_group)}_flocks"
            }
            merged_flocks.append(merged_flock)
        else:
            merged_flocks.append(flock1)
    
    return merged_flocks + others

def match_objects_yolo(results1, results2) -> List[Dict]:
    """
    ë‘ YOLO ê²°ê³¼ì—ì„œ í´ë˜ìŠ¤ë³„ë¡œ ê°ì²´ë¥¼ ë§¤ì¹­ (ë°°ì¹˜ ì²˜ë¦¬ìš©)
    Flockì˜ ê²½ìš° ê°€ê¹Œìš´ ê²ƒë“¤ë¼ë¦¬ í†µí•© í›„ ë§¤ì¹­
    """
    matches = []
    
    # Flockê³¼ ë‹¤ë¥¸ í´ë˜ìŠ¤ ë¶„ë¦¬
    flocks1 = []
    others1 = {}
    for b, c, conf in zip(results1.boxes.xywh.cpu(), results1.boxes.cls.cpu(), results1.boxes.conf.cpu()):
        cls_name = results1.names[int(c)]
        if cls_name == "Flock":
            flocks1.append((b, conf))
        else:
            others1[cls_name] = (b, conf)
    
    flocks2 = []
    others2 = {}
    for b, c, conf in zip(results2.boxes.xywh.cpu(), results2.boxes.cls.cpu(), results2.boxes.conf.cpu()):
        cls_name = results2.names[int(c)]
        if cls_name == "Flock":
            flocks2.append((b, conf))
        else:
            others2[cls_name] = (b, conf)
    
    # Flock í†µí•©
    merged_flocks1 = merge_nearby_flocks_2d(flocks1)
    merged_flocks2 = merge_nearby_flocks_2d(flocks2)
    
    # í†µí•©ëœ Flock ë§¤ì¹­
    for box1, conf1 in merged_flocks1:
        for box2, conf2 in merged_flocks2:
            pt1 = np.array([box1[0], box1[1]], dtype=np.float32)
            pt2 = np.array([box2[0], box2[1]], dtype=np.float32)
            
            matches.append({
                'class': "Flock",
                'pt1': pt1,
                'pt2': pt2,
                'conf1': conf1,
                'conf2': conf2
            })
    
    # ë‹¤ë¥¸ í´ë˜ìŠ¤ ë§¤ì¹­
    common_classes = set(others1.keys()) & set(others2.keys())
    for cls_name in common_classes:
        box1, conf1 = others1[cls_name]
        box2, conf2 = others2[cls_name]
        
        pt1 = np.array([box1[0], box1[1]], dtype=np.float32)
        pt2 = np.array([box2[0], box2[1]], dtype=np.float32)
        
        matches.append({
            'class': cls_name,
            'pt1': pt1,
            'pt2': pt2,
            'conf1': conf1,
            'conf2': conf2
        })
    
    return matches

def match_objects_simple(detections1: List[Dict], detections2: List[Dict]) -> List[Dict]:
    """
    ë‘ ì¹´ë©”ë¼ì˜ ê°ì§€ ê²°ê³¼ ë§¤ì¹­ (ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ìš©) - ê°œì„ ëœ ë²„ì „
    
    Args:
        detections1, detections2: [{'class': str, 'center': [x,y], 'confidence': float}, ...]
    
    Returns:
        ë§¤ì¹­ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    matches = []
    used_det2_indices = set()  # ì´ë¯¸ ë§¤ì¹­ëœ det2 ì¸ë±ìŠ¤ ì¶”ì 
    
    for det1 in detections1:
        for i, det2 in enumerate(detections2):
            # ì´ë¯¸ ë§¤ì¹­ëœ det2ëŠ” ê±´ë„ˆë›°ê¸°
            if i in used_det2_indices:
                continue
                
            if det1['class'] == det2['class']:
                matches.append({
                    'class': det1['class'],
                    'det1': det1,
                    'det2': det2
                })
                used_det2_indices.add(i)  # ë§¤ì¹­ëœ det2 ì¸ë±ìŠ¤ ê¸°ë¡
                break  # ì´ det1ì— ëŒ€í•œ ë§¤ì¹­ ì™„ë£Œ
    
    return matches

# =========================
# ğŸ”º ì‚¼ê°ì¸¡ëŸ‰ í•µì‹¬ í•¨ìˆ˜
# =========================

def triangulate_point(point1: List[float], point2: List[float], 
                     P1: np.ndarray, P2: np.ndarray,
                     camera_positions: List[np.ndarray] = None) -> Optional[np.ndarray]:
    """
    ì˜¬ë°”ë¥¸ ì‚¼ê°ì¸¡ëŸ‰ (ì¢Œí‘œê³„ ë³€í™˜ ì—†ì´ ì§ì ‘ ê³„ì‚°)
    
    Args:
        point1, point2: ì´ë¯¸ì§€ ì¢Œí‘œ [x, y]
        P1, P2: ì¹´ë©”ë¼ íˆ¬ì˜ í–‰ë ¬
        camera_positions: ì¹´ë©”ë¼ Unity ì›”ë“œ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    
    Returns:
        Unity ì›”ë“œ ì¢Œí‘œ 3D ìœ„ì¹˜ [x, y, z] ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
    """
    try:
        # ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ homogeneous í˜•íƒœë¡œ ë³€í™˜
        points1 = np.array([[point1[0]], [point1[1]]], dtype=np.float32)
        points2 = np.array([[point2[0]], [point2[1]]], dtype=np.float32)
        
        # OpenCV ì‚¼ê°ì¸¡ëŸ‰
        points_4d_hom = cv2.triangulatePoints(P1, P2, points1, points2)
        
        # Homogeneous â†’ 3D ì¢Œí‘œ ë³€í™˜
        if points_4d_hom[3, 0] != 0:
            points_3d = points_4d_hom[:3, 0] / points_4d_hom[3, 0]
        else:
            return None
        
        # ê²°ê³¼ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ Unity ì›”ë“œ ì¢Œí‘œê³„
        print(f"ğŸ”„ ì‚¼ê°ì¸¡ëŸ‰ ê²°ê³¼: Unity({points_3d[0]:.1f}, {points_3d[1]:.1f}, {points_3d[2]:.1f})")
        
        return points_3d
        
    except Exception as e:
        print(f"âŒ ì‚¼ê°ì¸¡ëŸ‰ ì˜¤ë¥˜: {e}")
        return None

def triangulate_objects_realtime(detections: List[Dict], 
                                projection_matrices: List[np.ndarray],
                                camera_letters: List[str],
                                frame_id: int,
                                distance_threshold: float = 100) -> List[Dict]:
    """
    ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ìš© ê°ì²´ ì‚¼ê°ì¸¡ëŸ‰
    
    Args:
        detections: [{'camera': str, 'class': str, 'center': [x,y], 'confidence': float}, ...]
        projection_matrices: ì¹´ë©”ë¼ íˆ¬ì˜ í–‰ë ¬ ë¦¬ìŠ¤íŠ¸
        camera_letters: ì¹´ë©”ë¼ ë¬¸ì ë¦¬ìŠ¤íŠ¸ ['A', 'B', 'C', 'D']
        frame_id: í”„ë ˆì„ ID
        distance_threshold: ê·¼ì ‘ ë¬´ë¦¬ ë³‘í•© ì„ê³„ê°’
    
    Returns:
        ì‚¼ê°ì¸¡ëŸ‰ëœ 3D ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    if len(projection_matrices) < 2:
        return []
    
    # ì¹´ë©”ë¼ë³„ë¡œ ê°ì§€ ê²°ê³¼ ê·¸ë£¹í™”
    camera_detections = {}
    for det in detections:
        camera = det['camera']
        if camera not in camera_detections:
            camera_detections[camera] = []
        camera_detections[camera].append(det)
    
    triangulated_points = []
    
    # ì¹´ë©”ë¼ ìŒë³„ë¡œ ì‚¼ê°ì¸¡ëŸ‰ ìˆ˜í–‰
    available_cameras = list(camera_detections.keys())
    for i in range(len(available_cameras)):
        for j in range(i + 1, len(available_cameras)):
            cam1, cam2 = available_cameras[i], available_cameras[j]
            cam1_idx = camera_letters.index(cam1)
            cam2_idx = camera_letters.index(cam2)
            
            # ê°ì²´ ë§¤ì¹­
            matches = match_objects_simple(
                camera_detections[cam1], 
                camera_detections[cam2]
            )
            
            for match in matches:
                # ì‚¼ê°ì¸¡ëŸ‰ ìˆ˜í–‰
                point_3d = triangulate_point(
                    match['det1']['center'],
                    match['det2']['center'],
                    projection_matrices[cam1_idx],
                    projection_matrices[cam2_idx]
                )
                
                if point_3d is not None:
                    triangulated_points.append({
                        'frame': frame_id,
                        'class': match['class'],
                        'x': point_3d[0],
                        'y': point_3d[1],
                        'z': point_3d[2],
                        'confidence': (match['det1']['confidence'] + match['det2']['confidence']) / 2,
                        'cameras': f'Camera_{cam1}-Camera_{cam2}'
                    })
    
    # ê·¼ì ‘í•œ ë¬´ë¦¬ ë³‘í•©
    if triangulated_points:
        triangulated_points = merge_nearby_flocks_3d(triangulated_points, distance_threshold)
    
    return triangulated_points

def process_frame_multicam(img_paths: List[Path], 
                          aviation_detector: AviationDetector, 
                          projection_matrices: List[np.ndarray],
                          camera_positions: List[np.ndarray] = None) -> List[Dict]:
    """
    ì—¬ëŸ¬ ì¹´ë©”ë¼ì˜ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ì—¬ 3D ìœ„ì¹˜ë¥¼ ì¶”ì • (ë°°ì¹˜ ì²˜ë¦¬ìš©)
    2ê°œ ì´ìƒì˜ ì¹´ë©”ë¼ì—ì„œ ì‘ë™
    
    Args:
        img_paths: ì¹´ë©”ë¼ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (2ê°œ ì´ìƒ)
        aviation_detector: í•­ê³µ ê°ì§€ê¸° (í†µí•© ëª¨ë“ˆ)
        projection_matrices: ì¹´ë©”ë¼ íˆ¬ì˜ í–‰ë ¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì‚¼ê°ì¸¡ëŸ‰ëœ 3D ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    num_cameras = len(img_paths)
    if num_cameras < 2:
        print("âŒ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì¹´ë©”ë¼ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return []
    
    # 1. ê° ì¹´ë©”ë¼ì—ì„œ ê°ì²´ ê°ì§€
    detections = []
    valid_cameras = []  # ê°ì§€ê°€ ì„±ê³µí•œ ì¹´ë©”ë¼ ì¸ë±ìŠ¤ ì €ì¥
    
    for i, img_path in enumerate(img_paths):
        try:
            # ğŸ¯ AviationDetectorë¡œ ê°ì²´ ê°ì§€
            detected_objects = aviation_detector.detect_single_image(img_path, return_raw=True)
            if detected_objects['detections']:  # ê°ì²´ê°€ ê°ì§€ëœ ê²½ìš°
                detections.append(detected_objects['raw_results'][0])  # YOLO ì›ì‹œ ê²°ê³¼ ì‚¬ìš©
                valid_cameras.append(i)
            else:
                print(f"âš ï¸ Camera_{chr(65+i)}ì—ì„œ ê°ì²´ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ")
                detections.append(None)
        except Exception as e:
            print(f"âŒ Camera_{chr(65+i)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            detections.append(None)
    
    if len(valid_cameras) < 2:
        print("âš ï¸ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì¹´ë©”ë¼ì—ì„œ ê°ì²´ê°€ ê°ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        return []
    
    # 2. ê°ì§€ëœ ì¹´ë©”ë¼ë“¤ ê°„ì˜ ì¡°í•©ìœ¼ë¡œ ì‚¼ê°ì¸¡ëŸ‰ ìˆ˜í–‰
    triangulated_points = []
    for i in range(len(valid_cameras)):
        for j in range(i+1, len(valid_cameras)):
            cam1_idx = valid_cameras[i]
            cam2_idx = valid_cameras[j]
            
            matches = match_objects_yolo(detections[cam1_idx], detections[cam2_idx])
            if not matches:
                continue
                
            for match in matches:
                # ì˜¬ë°”ë¥¸ ì‚¼ê°ì¸¡ëŸ‰ í•¨ìˆ˜ ì‚¬ìš©
                point_3d_unity = triangulate_point(
                    match['pt1'].flatten().tolist(),
                    match['pt2'].flatten().tolist(),
                    projection_matrices[cam1_idx],
                    projection_matrices[cam2_idx]
                )
                
                if point_3d_unity is None:
                    continue
                
                # ì´ìƒê°’ í•„í„°ë§ (ë¹„ì •ìƒì ìœ¼ë¡œ í° ì¢Œí‘œê°’ ì œê±°)
                max_coord = 10000  # ìµœëŒ€ í—ˆìš© ì¢Œí‘œê°’
                if (abs(point_3d_unity[0]) > max_coord or 
                    abs(point_3d_unity[1]) > max_coord or 
                    abs(point_3d_unity[2]) > max_coord):
                    print(f"âš ï¸ ì´ìƒê°’ ì œê±°: {match['class']} at ({point_3d_unity[0]:.1f}, {point_3d_unity[1]:.1f}, {point_3d_unity[2]:.1f})")
                    continue
                
                triangulated_points.append({
                    'frame': int(img_paths[0].stem.split('_')[1]),
                    'class': match['class'],
                    'x': point_3d_unity[0],
                    'y': point_3d_unity[1],
                    'z': point_3d_unity[2],
                    'confidence': (match['conf1'].item() + match['conf2'].item()) / 2,
                    'cameras': f'Camera_{chr(65+cam1_idx)}-Camera_{chr(65+cam2_idx)}',
                    'num_detected_cameras': len(valid_cameras),
                    'total_cameras': num_cameras
                })
    
    # 3. ì¤‘ë³µ ì œê±° ë° í‰ê· í™”
    merged_points = []
    frame_class_groups = {}
    
    for point in triangulated_points:
        key = (point['frame'], point['class'])
        if key not in frame_class_groups:
            frame_class_groups[key] = []
        frame_class_groups[key].append(point)
    
    for points in frame_class_groups.values():
        if len(points) > 0:
            # ìœ„ì¹˜ í‰ê· í™”
            avg_x = np.mean([p['x'] for p in points])
            avg_y = np.mean([p['y'] for p in points])
            avg_z = np.mean([p['z'] for p in points])
            avg_conf = np.mean([p['confidence'] for p in points])
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°ì§€ëœ ì¹´ë©”ë¼ ìˆ˜ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
            max_detected_cameras = max(p['num_detected_cameras'] for p in points)
            confidence_weight = max_detected_cameras / num_cameras
            
            merged_points.append({
                'frame': points[0]['frame'],
                'class': points[0]['class'],
                'x': avg_x,
                'y': avg_y,
                'z': avg_z,
                'confidence': avg_conf * confidence_weight,
                'num_detected_cameras': max_detected_cameras,
                'total_cameras': num_cameras,
                'num_camera_pairs': len(points)
            })
    
    return merged_points

# =========================
# ğŸ’¾ ê²°ê³¼ ì €ì¥ ë° ìœ í‹¸ë¦¬í‹°
# =========================

def save_results(results: List[Dict], output_dir: Path):
    """ê²°ê³¼ë¥¼ CSVì™€ JSONìœ¼ë¡œ ì €ì¥"""
    if not results:
        print("âš ï¸ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # CSV ì €ì¥ì„ ìœ„í•´ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(results)
    
    # CSV ì €ì¥
    csv_path = output_dir / 'triangulation_results.csv'
    df.to_csv(csv_path, index=False)
    print(f"âœ… CSV ê²°ê³¼ ì €ì¥: {csv_path}")
    
    # JSON ì €ì¥ì„ ìœ„í•´ NumPy/Tensor íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    json_serializable_results = []
    for res in results:
        serializable_res = {}
        for k, v in res.items():
            if isinstance(v, (np.ndarray, np.generic)):
                serializable_res[k] = v.tolist()
            elif hasattr(v, 'item'): # PyTorch Tensor
                serializable_res[k] = v.item()
            else:
                serializable_res[k] = v
        json_serializable_results.append(serializable_res)

    # JSON ì €ì¥ (í”„ë ˆì„ë³„ë¡œ êµ¬ì¡°í™”)
    json_results = {}
    for result in json_serializable_results:
        frame = result['frame']
        if frame not in json_results:
            json_results[frame] = []
        json_results[frame].append({
            'class': result['class'],
            'position': [result['x'], result['y'], result['z']],
            'confidence': result['confidence']
        })
    
    json_path = output_dir / 'triangulation_results.json'
    with open(json_path, 'w') as f:
        json.dump(json_results, f, indent=2)
    print(f"âœ… JSON ê²°ê³¼ ì €ì¥: {json_path}")

def find_latest_folder(base_path: Path, pattern: str) -> Optional[Path]:
    """ì§€ì •ëœ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ì¥ ìµœì‹  í´ë”ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    folders = list(Path(base_path).glob(pattern))
    if not folders:
        return None
    return max(folders, key=lambda p: p.stat().st_mtime)

# =========================
# ğŸ“¡ ë©”ì¸ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬)
# =========================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë…ë¦½ ì‹¤í–‰ìš©)"""
    print("ğŸš€ 3D Triangulation ì‹œì‘...")
    
    # ê²½ë¡œ ì„¤ì •
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent  # scripts/ -> BirdRiskSim_v2/

    # ê°€ì¥ ìµœì‹  ë°ì´í„° í´ë” ì°¾ê¸°
    latest_data_folder = find_latest_folder(project_root / "data/sync_capture", "Recording_*")
    if not latest_data_folder:
        print("âŒ 'data/sync_capture'ì—ì„œ ë…¹í™” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    data_folder = latest_data_folder

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ ìë™ ê°ì§€
    available_cameras = []
    camera_params = []
    projection_matrices = []
    camera_positions = []  # ğŸ¯ ì¹´ë©”ë¼ Unity ì›”ë“œ ìœ„ì¹˜ ì €ì¥
    stereo_calibrations = {}  # ğŸ¯ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì €ì¥
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ì¹´ë©”ë¼ ë¬¸ì í™•ì¸ (Camera_* ë° Fixed_Camera_* íŒ¨í„´ ì§€ì›)
    camera_patterns = ["Camera_{}", "Fixed_Camera_{}", "Fixed_Camera_{}_2"]
    
    for letter in ['A', 'B', 'C', 'D', 'G', 'H']:
        camera_found = False
        
        for pattern in camera_patterns:
            params_path = data_folder / f"{pattern.format(letter)}_parameters.json"
            if params_path.exists():
                try:
                    params = load_camera_parameters(params_path)
                    P = get_projection_matrix(params)
                    
                    # ğŸ¯ ì¹´ë©”ë¼ Unity ì›”ë“œ ìœ„ì¹˜ ì¶”ì¶œ
                    camera_pos = np.array([
                        params['position_UnityWorld']['x'],
                        params['position_UnityWorld']['y'],
                        params['position_UnityWorld']['z']
                    ])
                    
                    available_cameras.append(letter)
                    camera_params.append(params)
                    projection_matrices.append(P)
                    camera_positions.append(camera_pos)
                    print(f"  âœ… {pattern.format(letter)} íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ - ìœ„ì¹˜: ({camera_pos[0]:.1f}, {camera_pos[1]:.1f}, {camera_pos[2]:.1f})")
                    camera_found = True
                    break
                except Exception as e:
                    print(f"  âš ï¸ {pattern.format(letter)} íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if not camera_found:
            print(f"  âš ï¸ Camera_{letter} íŒŒë¼ë¯¸í„° íŒŒì¼ ì—†ìŒ")
    
    # ğŸ¯ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚°
    if len(available_cameras) >= 2:
        print("\nğŸ”§ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ì¤‘...")
        for i in range(len(available_cameras)):
            for j in range(i + 1, len(available_cameras)):
                cam1_letter = available_cameras[i]
                cam2_letter = available_cameras[j]
                
                stereo_calib = calculate_stereo_calibration(camera_params[i], camera_params[j])
                stereo_calibrations[f"{cam1_letter}-{cam2_letter}"] = stereo_calib
                
                baseline = stereo_calib['baseline']
                print(f"  ğŸ“ Camera_{cam1_letter}-Camera_{cam2_letter}: ë² ì´ìŠ¤ë¼ì¸ {baseline:.1f}m")
    else:
        print("âš ï¸ ìŠ¤í…Œë ˆì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ìœ„í•œ ì¶©ë¶„í•œ ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if len(available_cameras) < 2:
        print(f"âŒ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì¹´ë©”ë¼ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ {len(available_cameras)}ê°œ ë°œê²¬")
        return
    
    print(f"ğŸ“· ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼: {', '.join([f'Camera_{c}' for c in available_cameras])} ({len(available_cameras)}ê°œ)")
    
    # ğŸ¯ í•­ê³µ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í†µí•© ëª¨ë“ˆ ì‚¬ìš©)
    print("ğŸ¤– í•­ê³µ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    aviation_detector = AviationDetector()
    
    if aviation_detector.model is None:
        print("âŒ í•­ê³µ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    print("âœ… í•­ê³µ ê°ì§€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")

    # ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬
    all_results = []
    
    # ì²« ë²ˆì§¸ ì¹´ë©”ë¼ì˜ ì‹¤ì œ í´ë”ëª… ì°¾ê¸°
    first_camera_letter = available_cameras[0]
    camera_folder_name = None
    
    for pattern in camera_patterns:
        folder_name = pattern.format(first_camera_letter)
        if (data_folder / folder_name).exists():
            camera_folder_name = folder_name
            break
    
    if not camera_folder_name:
        print(f"âŒ ì²« ë²ˆì§¸ ì¹´ë©”ë¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: Camera_{first_camera_letter}")
        return
    
    frame_files = sorted(glob.glob(str(data_folder / f"{camera_folder_name}/*.jpg")))
    total_frames = len(frame_files)
    
    print(f"\nğŸ“Š ì´ {total_frames}ê°œ í”„ë ˆì„ ì²˜ë¦¬ ì‹œì‘...")
    print(f"  - ì‚¬ìš© ì¹´ë©”ë¼: {', '.join([f'Camera_{c}' for c in available_cameras])}")
    
    for i, frame_path in enumerate(frame_files):
        if (i + 1) % 10 == 0:
            print(f"  ì²˜ë¦¬ ì¤‘: {i+1}/{total_frames}")
        
        frame_name = Path(frame_path).name
        
        # ê° ì¹´ë©”ë¼ì˜ ì‹¤ì œ í´ë”ëª…ìœ¼ë¡œ ì´ë¯¸ì§€ ê²½ë¡œ êµ¬ì„±
        img_paths = []
        for letter in available_cameras:
            for pattern in camera_patterns:
                folder_name = pattern.format(letter)
                img_path = data_folder / folder_name / frame_name
                if img_path.exists():
                    img_paths.append(img_path)
                    break
            else:
                # í•´ë‹¹ ì¹´ë©”ë¼ì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
                img_paths = []
                break
        
        # ëª¨ë“  ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if len(img_paths) != len(available_cameras):
            print(f"âš ï¸  ì¼ë¶€ ì¹´ë©”ë¼ì˜ í”„ë ˆì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {frame_name}")
            continue
        
        frame_results = process_frame_multicam(img_paths, aviation_detector, projection_matrices, camera_positions)
        all_results.extend(frame_results)
    

        
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    output_dir = project_root / "data" / "triangulation_results" / f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    output_dir.mkdir(parents=True, exist_ok=True)
    save_results(all_results, output_dir)
    
    print("\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œì˜ 3D ìœ„ì¹˜ ì¶”ì • ì™„ë£Œ")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")







if __name__ == "__main__":
    main() 